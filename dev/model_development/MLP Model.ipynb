{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee8665d-cd07-4089-83a1-4b0c6bc75e65",
   "metadata": {},
   "source": [
    "# MLP Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9f1a23d-c2f7-4680-bf1e-2b8e1b2f114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad6cdd97-5982-4daf-ba3a-3cde430ca6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLP Model\") \\\n",
    "    .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57ee1627-228d-4db9-ac61-657773aa6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = \"gs://msca-bdp-student-gcs/Group_1_final_project/curated/final_features\" \n",
    "events_taxi_df = spark.read.parquet(final_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04f513-2723-4255-a18d-64524c0809a1",
   "metadata": {},
   "source": [
    "**Get the most busy zone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cd4c8d8-b51b-4d1f-9687-e3d206036859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-zone 90th percentile of trip_count\n",
    "zone_q = (\n",
    "    events_taxi_df\n",
    "    .groupBy(\"PULocationID\")\n",
    "    .agg(F.expr(\"percentile_approx(trip_count, 0.90)\").alias(\"q90\"))\n",
    ")\n",
    "\n",
    "events_taxi_surge = events_taxi_df.join(zone_q, \"PULocationID\", \"left\")\n",
    "\n",
    "# Binary surge label: 1 if trip_count > zone-specific 90th percentile\n",
    "events_taxi_surge = events_taxi_surge.withColumn(\n",
    "    \"is_surge\",\n",
    "    (F.col(\"trip_count\") > F.col(\"q90\")).cast(\"int\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a307da-196e-4371-8b11-8480c6e13eeb",
   "metadata": {},
   "source": [
    "### Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "312ba511-7422-4119-8396-df09f0ba7160",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"is_surge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a735e49c-4f04-45ea-9af7-430e422ae1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"borough\",\n",
    "    \"event_borough\",\n",
    "    \"weather_borough\",\n",
    "    \"station_name\",\n",
    "    \"season\",\n",
    "    \"weather_date\",\n",
    "    \"date\"\n",
    "]\n",
    "\n",
    "events_taxi_surge_final = events_taxi_surge.drop(*[c for c in drop_cols if c in events_taxi_surge.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de43f8-a6f4-4866-be78-fe9e641d4f34",
   "metadata": {},
   "source": [
    "### Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed4e72b2-d022-49e9-929d-0ff79f1e31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"PULocationID\").orderBy(\"half_hour\")\n",
    "\n",
    "events_taxi_surge_final = (\n",
    "    events_taxi_surge_final\n",
    "    .withColumn(\"lag1_trip_count\", F.lag(\"trip_count\", 1).over(w))\n",
    "    .withColumn(\"lag2_trip_count\", F.lag(\"trip_count\", 2).over(w))\n",
    "    .withColumn(\"lag4_trip_count\", F.lag(\"trip_count\", 4).over(w))\n",
    "    .withColumn(\"rolling_mean_2hr\",\n",
    "        F.avg(\"trip_count\").over(w.rowsBetween(-4, -1))\n",
    "    )\n",
    ")\n",
    "\n",
    "events_taxi_surge_final = events_taxi_surge_final.fillna(\n",
    "    {\n",
    "        \"lag1_trip_count\": 0.0,\n",
    "        \"lag2_trip_count\": 0.0,\n",
    "        \"lag4_trip_count\": 0.0,\n",
    "        \"rolling_mean_2hr\": 0.0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9280e13c-9afb-47ed-b068-11048b93cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no null labels\n",
    "events_taxi_surge_final = (\n",
    "    events_taxi_surge_final\n",
    "    .filter(F.col(\"trip_count\").isNotNull())\n",
    "    .filter(F.col(\"is_surge\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a3ea435-36b0-4cd1-bfcb-67b6956de4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing nulls in numeric columns: 56\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric columns to impute (exclude labels)\n",
    "numeric_impute_cols = [\n",
    "    c for c, t in events_taxi_surge_final.dtypes\n",
    "    if t in (\"int\", \"bigint\", \"double\") and c not in (\"trip_count\", \"is_surge\")\n",
    "]\n",
    "\n",
    "print(\"Imputing nulls in numeric columns:\", len(numeric_impute_cols))\n",
    "\n",
    "events_taxi_surge_final = events_taxi_surge_final.fillna(0, subset=numeric_impute_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1890ae6-9752-4431-9f16-88fdec84ce93",
   "metadata": {},
   "source": [
    "### Train, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85a8e7e5-cf9c-4740-a2d4-8ac3e66a85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff = \"2024-01-01\"\n",
    "test_cutoff   = \"2024-03-01\"\n",
    "\n",
    "train_df = events_taxi_surge_final.filter(F.col(\"half_hour\") < train_cutoff)\n",
    "test_df  = events_taxi_surge_final.filter(F.col(\"half_hour\") >= test_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9c0d8-c703-47fc-8618-b55aee72d450",
   "metadata": {},
   "source": [
    "**Compute Class weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "218e27c1-4b28-4724-85f8-40f03ebf48d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts before balancing: {'0': 1412253, '1': 109374}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:===================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts after balancing: {'0': 328489, '1': 109374}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "label_col = \"is_surge\"\n",
    "\n",
    "# Separate surge / non-surge in train\n",
    "train_surge = train_df.filter(F.col(label_col) == 1)\n",
    "train_nonsurge = train_df.filter(F.col(label_col) == 0)\n",
    "\n",
    "n_pos = train_surge.count()\n",
    "n_neg = train_nonsurge.count()\n",
    "\n",
    "print(\"Train counts before balancing:\", {\"0\": n_neg, \"1\": n_pos})\n",
    "\n",
    "# e.g. keep about 3x as many non-surge as surge\n",
    "target_ratio = 3.0\n",
    "fraction_neg = min(1.0, (target_ratio * n_pos) / float(n_neg))\n",
    "\n",
    "train_nonsurge_down = train_nonsurge.sample(\n",
    "    withReplacement=False,\n",
    "    fraction=fraction_neg,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_balanced = train_surge.union(train_nonsurge_down)\n",
    "\n",
    "n_pos_b = train_balanced.filter(F.col(label_col) == 1).count()\n",
    "n_neg_b = train_balanced.filter(F.col(label_col) == 0).count()\n",
    "print(\"Train counts after balancing:\", {\"0\": n_neg_b, \"1\": n_pos_b})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b4236-bb01-4b74-8c0c-2dc187b82c97",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05da3c22-bee1-4e74-847c-d224aea7afab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PULocationID', 'int'),\n",
       " ('half_hour', 'timestamp'),\n",
       " ('trip_count', 'bigint'),\n",
       " ('total_passengers', 'bigint'),\n",
       " ('avg_passenger_count', 'double'),\n",
       " ('avg_trip_distance', 'double'),\n",
       " ('avg_speed_mph', 'double'),\n",
       " ('avg_fare_amount', 'double'),\n",
       " ('avg_total_amount', 'double'),\n",
       " ('avg_tip_rate', 'double'),\n",
       " ('avg_fare_per_mile', 'double'),\n",
       " ('sum_total_fees', 'double'),\n",
       " ('num_high_ppm_trips', 'bigint'),\n",
       " ('num_extreme_speed_trips', 'bigint'),\n",
       " ('num_near_station_pickups', 'bigint'),\n",
       " ('avg_hour_sin', 'double'),\n",
       " ('avg_hour_cos', 'double'),\n",
       " ('day_of_week', 'int'),\n",
       " ('is_weekend', 'int'),\n",
       " ('month', 'int'),\n",
       " ('hour_of_day', 'int'),\n",
       " ('total_events', 'bigint'),\n",
       " ('avg_event_duration_min', 'double'),\n",
       " ('total_event_importance', 'double'),\n",
       " ('event_start_flag', 'int'),\n",
       " ('event_end_flag', 'int'),\n",
       " ('miscellaneous', 'bigint'),\n",
       " ('lat', 'double'),\n",
       " ('lon', 'double'),\n",
       " ('elevation_m', 'double'),\n",
       " ('temp_avg', 'double'),\n",
       " ('dew_point', 'double'),\n",
       " ('pressure_sea_level', 'double'),\n",
       " ('visibility_mi', 'double'),\n",
       " ('wind_speed_avg', 'double'),\n",
       " ('wind_speed_max', 'double'),\n",
       " ('wind_gust', 'double'),\n",
       " ('temp_max', 'double'),\n",
       " ('temp_min', 'double'),\n",
       " ('precip_in', 'double'),\n",
       " ('snow_depth_in', 'double'),\n",
       " ('weather_flags', 'int'),\n",
       " ('is_precip', 'int'),\n",
       " ('is_heavy_rain', 'int'),\n",
       " ('is_snow', 'int'),\n",
       " ('is_low_visibility', 'int'),\n",
       " ('is_windy', 'int'),\n",
       " ('is_storm_gust', 'int'),\n",
       " ('temp_range', 'double'),\n",
       " ('wind_ratio', 'double'),\n",
       " ('wind_severity', 'double'),\n",
       " ('is_cold_snap', 'int'),\n",
       " ('is_heat_wave', 'int'),\n",
       " ('q90', 'bigint'),\n",
       " ('is_surge', 'int'),\n",
       " ('lag1_trip_count', 'bigint'),\n",
       " ('lag2_trip_count', 'bigint'),\n",
       " ('lag4_trip_count', 'bigint'),\n",
       " ('rolling_mean_2hr', 'double')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_taxi_surge_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1bd27cc5-bd17-47ec-b63c-8785efa35432",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feats = [\n",
    "    \"PULocationID\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"hour_of_day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60700213-71eb-4b24-b0d5-2a43ad0e540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\n",
    "    # taxi stats (no trip_count)\n",
    "    \"total_passengers\",\n",
    "    \"avg_passenger_count\",\n",
    "    \"avg_trip_distance\",\n",
    "    \"avg_speed_mph\",\n",
    "    \"avg_fare_amount\",\n",
    "    \"avg_total_amount\",\n",
    "    \"avg_tip_rate\",\n",
    "    \"avg_fare_per_mile\",\n",
    "    \"sum_total_fees\",\n",
    "    \"num_high_ppm_trips\",\n",
    "    \"num_extreme_speed_trips\",\n",
    "    \"num_near_station_pickups\",\n",
    "\n",
    "    # time encodings\n",
    "    \"avg_hour_sin\",\n",
    "    \"avg_hour_cos\",\n",
    "\n",
    "    # events\n",
    "    \"total_events\",\n",
    "    \"avg_event_duration_min\",\n",
    "    \"total_event_importance\",\n",
    "    \"miscellaneous\",\n",
    "\n",
    "    # geo\n",
    "    \"lat\", \"lon\", \"elevation_m\",\n",
    "\n",
    "    # weather continuous\n",
    "    \"temp_avg\", \"dew_point\", \"pressure_sea_level\",\n",
    "    \"visibility_mi\", \"wind_speed_avg\", \"wind_speed_max\",\n",
    "    \"wind_gust\", \"temp_max\", \"temp_min\",\n",
    "    \"precip_in\", \"snow_depth_in\",\n",
    "    \"temp_range\", \"wind_ratio\", \"wind_severity\",\n",
    "\n",
    "    # lags / rolling\n",
    "    \"lag1_trip_count\",\n",
    "    \"lag2_trip_count\",\n",
    "    \"lag4_trip_count\",\n",
    "    \"rolling_mean_2hr\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44ec1f2e-6afb-4a43-8c10-a36054fe0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_feats = [\n",
    "    \"is_weekend\",\n",
    "    \"event_start_flag\",\n",
    "    \"event_end_flag\",\n",
    "    \"is_precip\",\n",
    "    \"is_heavy_rain\",\n",
    "    \"is_snow\",\n",
    "    \"is_low_visibility\",\n",
    "    \"is_windy\",\n",
    "    \"is_storm_gust\",\n",
    "    \"is_cold_snap\",\n",
    "    \"is_heat_wave\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c06cd4-8501-43ce-bed8-2c1760255b6e",
   "metadata": {},
   "source": [
    "### Clean MLP Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c756f7fb-4314-4928-8d91-21e15b64822b",
   "metadata": {},
   "source": [
    "**Index + one-hot encode for categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c8b428d2-3ab4-41bc-b9ed-d414945361e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Index + one-hot for categoricals\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in categorical_feats\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(\n",
    "        inputCols=[f\"{c}_idx\"],\n",
    "        outputCols=[f\"{c}_oh\"]\n",
    "    )\n",
    "    for c in categorical_feats\n",
    "]\n",
    "\n",
    "# 2. Assemble and scale numeric (continuous + binary flags)\n",
    "numeric_assembler = VectorAssembler(\n",
    "    inputCols=numeric_feats + binary_feats,\n",
    "    outputCol=\"numeric_vector\"\n",
    ")\n",
    "\n",
    "numeric_scaler = StandardScaler(\n",
    "    inputCol=\"numeric_vector\",\n",
    "    outputCol=\"numeric_scaled\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "# 3. Final features vector: scaled numeric + raw one-hot\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"numeric_scaled\"] + [f\"{c}_oh\" for c in categorical_feats],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "feature_stages = indexers + encoders + [numeric_assembler, numeric_scaler, final_assembler]\n",
    "feature_pipeline = Pipeline(stages=feature_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383acd0-f4df-4101-8d1c-8a93eabbe77b",
   "metadata": {},
   "source": [
    "### Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3d306221-dc48-4233-93e8-4cb1fb171929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit on TRAIN ONLY\n",
    "feature_model = feature_pipeline.fit(train_balanced)\n",
    "\n",
    "# Transform train and test\n",
    "train_prepared = feature_model.transform(train_balanced)\n",
    "test_prepared  = feature_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "844cbd0e-531c-4132-b500-6760993b993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get feature dimension from one row\n",
    "first_vec = train_prepared.select(\"features\").first()[\"features\"]\n",
    "input_dim = len(first_vec)\n",
    "print(\"Input dimension:\", input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e98831-e0db-403e-b685-70e5ee98e92a",
   "metadata": {},
   "source": [
    "### MLP Classifer - Surge Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "698660dc-3a11-4bbf-825e-5a7d241cbe5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 02:22:58 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "25/12/07 02:22:58 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "[Stage 447:==================================================>    (23 + 2) / 25]\r"
     ]
    }
   ],
   "source": [
    "layers = [input_dim, 2 * input_dim, 2]   # simple 1-hidden-layer MLP\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    labelCol=label_col,\n",
    "    featuresCol=\"features\",\n",
    "    layers=layers,\n",
    "    blockSize=256,\n",
    "    maxIter=80,\n",
    "    stepSize=0.05,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "mlp_model = mlp.fit(train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d3f13f47-535a-41be-b6c4-da6e536d078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 02:35:12 WARN org.apache.spark.scheduler.TaskSetManager: Stage 449 contains a task of very large size (1908 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: gs://msca-bdp-student-gcs/Group_1_final_project/Workspace/Data_pipelines/mlp_model\n"
     ]
    }
   ],
   "source": [
    "model_path = \"gs://msca-bdp-student-gcs/Group_1_final_project/Workspace/Data_pipelines/mlp_model\"\n",
    "\n",
    "# Save the full trained pipeline model\n",
    "mlp_model.write().overwrite().save(model_path)\n",
    "print(\"Model saved at:\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e828017-34c2-4548-a111-efab109df510",
   "metadata": {},
   "source": [
    "**Evaluate on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4929b8-a9a3-4df2-b96e-196469b3cd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 02:35:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/12/07 02:36:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP AUC-ROC: 0.9606\n",
      "MLP AUC-PR : 0.8563\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp_model.transform(test_prepared)\n",
    "\n",
    "# 6.1 ROC AUC and PR AUC\n",
    "evaluator_roc = BinaryClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "auc_roc = evaluator_roc.evaluate(predictions)\n",
    "auc_pr  = evaluator_pr.evaluate(predictions)\n",
    "\n",
    "print(f\"MLP AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"MLP AUC-PR : {auc_pr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8894489c-30fa-4cd5-8438-c67c6f30f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 02:37:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/07 02:37:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/07 02:38:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/12/07 02:38:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "[Stage 526:====================================================>  (41 + 2) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP at threshold = 0.5\n",
      "TP: 83905, FP: 45353, TN: 708137, FN: 14180\n",
      "Precision: 0.6491\n",
      "Recall   : 0.8554\n",
      "F1       : 0.7381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "threshold = 0.5  # adjust based on your PR curve / business trade-off\n",
    "\n",
    "# Convert probability vector -> array, then take element 1 (class \"1\" = surge)\n",
    "predictions = predictions.withColumn(\n",
    "    \"probability_arr\",\n",
    "    vector_to_array(\"probability\")\n",
    ").withColumn(\n",
    "    \"prob_surge\",\n",
    "    F.col(\"probability_arr\")[1]\n",
    ").withColumn(\n",
    "    \"pred_surge_custom\",\n",
    "    F.when(F.col(\"prob_surge\") >= threshold, F.lit(1)).otherwise(F.lit(0))\n",
    ")\n",
    "\n",
    "tp = predictions.filter((F.col(label_col) == 1) & (F.col(\"pred_surge_custom\") == 1)).count()\n",
    "fp = predictions.filter((F.col(label_col) == 0) & (F.col(\"pred_surge_custom\") == 1)).count()\n",
    "tn = predictions.filter((F.col(label_col) == 0) & (F.col(\"pred_surge_custom\") == 0)).count()\n",
    "fn = predictions.filter((F.col(label_col) == 1) & (F.col(\"pred_surge_custom\") == 0)).count()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "print(f\"\\nMLP at threshold = {threshold}\")\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1       : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12c0ba-1ada-421a-b110-c816003b6405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

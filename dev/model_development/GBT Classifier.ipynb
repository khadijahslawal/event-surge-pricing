{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee8665d-cd07-4089-83a1-4b0c6bc75e65",
   "metadata": {},
   "source": [
    "# Baseline GBT Classifier Model - Surge Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6820db-431b-4d13-afab-809e41c64ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f1a23d-c2f7-4680-bf1e-2b8e1b2f114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6cdd97-5982-4daf-ba3a-3cde430ca6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final Project Advanced Model\") \\\n",
    "    .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ee1627-228d-4db9-ac61-657773aa6551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_path = \"gs://msca-bdp-student-gcs/Group_1_final_project/curated/final_features\" \n",
    "events_taxi_df = spark.read.parquet(final_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04f513-2723-4255-a18d-64524c0809a1",
   "metadata": {},
   "source": [
    "**Get the most busy zone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d513999-d5f1-4392-a320-2b77e3bcde2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_taxi_df.groupBy(\"PULocationID\").agg(\n",
    "#         F.sum(F.col(\"trip_count\")).alias(\"sum\")\n",
    "#     ).orderBy(\"sum\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd4c8d8-b51b-4d1f-9687-e3d206036859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-zone 90th percentile of trip_count\n",
    "zone_q = (\n",
    "    events_taxi_df\n",
    "    .groupBy(\"PULocationID\")\n",
    "    .agg(F.expr(\"percentile_approx(trip_count, 0.90)\").alias(\"q90\"))\n",
    ")\n",
    "\n",
    "events_taxi_surge = events_taxi_df.join(zone_q, \"PULocationID\", \"left\")\n",
    "\n",
    "# Binary surge label: 1 if trip_count > zone-specific 90th percentile\n",
    "events_taxi_surge = events_taxi_surge.withColumn(\n",
    "    \"is_surge\",\n",
    "    (F.col(\"trip_count\") > F.col(\"q90\")).cast(\"int\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a307da-196e-4371-8b11-8480c6e13eeb",
   "metadata": {},
   "source": [
    "### Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79aa0b45-b242-47e5-9c21-40053f924c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_surge|  count|\n",
      "+--------+-------+\n",
      "|       1| 221468|\n",
      "|       0|2310894|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df_surge has the column \"is_surge\" (0 or 1)\n",
    "class_counts = events_taxi_surge.groupBy(\"is_surge\").count()\n",
    "\n",
    "class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdce4e53-ba91-4345-bd66-c5bf3a77444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative (no surge): 2310894\n",
      "Positive (surge):    221468\n"
     ]
    }
   ],
   "source": [
    "counts = class_counts.rdd.collectAsMap()\n",
    "\n",
    "n_neg = counts.get(0, 0)\n",
    "n_pos = counts.get(1, 0)\n",
    "\n",
    "print(\"Negative (no surge):\", n_neg)\n",
    "print(\"Positive (surge):   \", n_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a03794-97dc-4ac4-8e0a-b9eca74681bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive class weight = 10.43443748080987\n"
     ]
    }
   ],
   "source": [
    "pos_weight = n_neg / n_pos\n",
    "print(\"Positive class weight =\", pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35be61e5-e30e-415a-be02-52f62599ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_taxi_surge = events_taxi_surge.withColumn(\n",
    "    \"class_weight\",\n",
    "    F.when(F.col(\"is_surge\") == 1, F.lit(pos_weight)).otherwise(F.lit(1.0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a735e49c-4f04-45ea-9af7-430e422ae1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"borough\",\n",
    "    \"event_borough\",\n",
    "    \"weather_borough\",\n",
    "    \"station_name\",\n",
    "    \"season\",\n",
    "    \"weather_date\",\n",
    "    \"date\"\n",
    "]\n",
    "\n",
    "events_taxi_surge_final = events_taxi_surge.drop(*[c for c in drop_cols if c in events_taxi_surge.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de43f8-a6f4-4866-be78-fe9e641d4f34",
   "metadata": {},
   "source": [
    "### Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed4e72b2-d022-49e9-929d-0ff79f1e31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"PULocationID\").orderBy(\"half_hour\")\n",
    "\n",
    "events_taxi_surge_final = (\n",
    "    events_taxi_surge_final\n",
    "    .withColumn(\"lag1_trip_count\", F.lag(\"trip_count\", 1).over(w))\n",
    "    .withColumn(\"lag2_trip_count\", F.lag(\"trip_count\", 2).over(w))\n",
    "    .withColumn(\"lag4_trip_count\", F.lag(\"trip_count\", 4).over(w))\n",
    "    .withColumn(\"rolling_mean_2hr\",\n",
    "        F.avg(\"trip_count\").over(w.rowsBetween(-4, -1))\n",
    "    )\n",
    ")\n",
    "\n",
    "events_taxi_surge_final = events_taxi_surge_final.fillna(\n",
    "    {\n",
    "        \"lag1_trip_count\": 0.0,\n",
    "        \"lag2_trip_count\": 0.0,\n",
    "        \"lag4_trip_count\": 0.0,\n",
    "        \"rolling_mean_2hr\": 0.0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9280e13c-9afb-47ed-b068-11048b93cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no null labels\n",
    "events_taxi_surge_final = (\n",
    "    events_taxi_surge_final\n",
    "    .filter(F.col(\"trip_count\").isNotNull())\n",
    "    .filter(F.col(\"is_surge\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770dbc25-7e00-4b10-b88a-da6b5c246144",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c9e222c-e594-427b-82fc-a988376b2763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing nulls in numeric columns: 57\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric columns to impute (exclude labels)\n",
    "numeric_impute_cols = [\n",
    "    c for c, t in events_taxi_surge_final.dtypes\n",
    "    if t in (\"int\", \"bigint\", \"double\") and c not in (\"trip_count\", \"is_surge\")\n",
    "]\n",
    "\n",
    "print(\"Imputing nulls in numeric columns:\", len(numeric_impute_cols))\n",
    "\n",
    "events_taxi_surge_final = events_taxi_surge_final.fillna(0, subset=numeric_impute_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40488383-da98-4097-8334-f6e1dd4a86fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: 54\n"
     ]
    }
   ],
   "source": [
    "# Columns that should NEVER go into features\n",
    "exclude_cols = {\n",
    "    \"trip_count\",      # regression label\n",
    "    \"is_surge\",        # classification label\n",
    "    \"PULocationID\",    # ID\n",
    "    \"half_hour\",       # timestamp key\n",
    "    \"q90\",             # threshold used to define is_surge\n",
    "    \"class_weight\"     # sample weight, not feature\n",
    "}\n",
    "\n",
    "# Build feature_cols ON THE FULL df_ts\n",
    "feature_cols = [\n",
    "    c for c, t in events_taxi_surge_final.dtypes\n",
    "    if t not in (\"string\", \"timestamp\") and c not in exclude_cols\n",
    "]\n",
    "\n",
    "print(\"Feature columns:\", len(feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b88f3d7c-20b2-4603-9ec8-d84a9cb5a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff = \"2024-01-01\"\n",
    "val_cutoff   = \"2024-03-01\"\n",
    "\n",
    "train_df = events_taxi_surge_final.filter(F.col(\"half_hour\") < train_cutoff)\n",
    "val_df   = events_taxi_surge_final.filter((F.col(\"half_hour\") >= train_cutoff) & (F.col(\"half_hour\") < val_cutoff))\n",
    "test_df  = events_taxi_surge_final.filter(F.col(\"half_hour\") >= val_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383acd0-f4df-4101-8d1c-8a93eabbe77b",
   "metadata": {},
   "source": [
    "### Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "844cbd0e-531c-4132-b500-6760993b993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"   # extra guardrail\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e98831-e0db-403e-b685-70e5ee98e92a",
   "metadata": {},
   "source": [
    "### GBT Classifer - Surge Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698660dc-3a11-4bbf-825e-5a7d241cbe5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbt_cls = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"is_surge\",\n",
    "    weightCol=\"class_weight\",   # key for imbalance\n",
    "    maxDepth=8,\n",
    "    maxIter=80,\n",
    "    stepSize=0.05,\n",
    "    subsamplingRate=0.8\n",
    ")\n",
    "\n",
    "gbt_cls_pipeline = Pipeline(stages=[assembler, gbt_cls])\n",
    "gbt_cls_model = gbt_cls_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3f13f47-535a-41be-b6c4-da6e536d078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: gs://msca-bdp-student-gcs/Group_1_final_project/Workspace/Data_pipelines/gbt_surge_model\n"
     ]
    }
   ],
   "source": [
    "model_path = \"gs://msca-bdp-student-gcs/Group_1_final_project/Workspace/Data_pipelines/gbt_surge_model\"\n",
    "\n",
    "# Save the full trained pipeline model\n",
    "gbt_cls_model.write().overwrite().save(model_path)\n",
    "print(\"Model saved at:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df7d5d4c-ea45-4325-8a38-454c9f7b3c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 16:45:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTClassifier – Validation ROC AUC: 0.955794165430602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 16:45:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTClassifier – Validation PR AUC: 0.7339739328469862\n"
     ]
    }
   ],
   "source": [
    "# ROC AUC\n",
    "evaluator_roc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_surge\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "val_auc = evaluator_roc.evaluate(val_preds_cls)\n",
    "print(\"GBTClassifier – Validation ROC AUC:\", val_auc)\n",
    "\n",
    "# PR AUC\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_surge\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "val_pr_auc = evaluator_pr.evaluate(val_preds_cls)\n",
    "print(\"GBTClassifier – Validation PR AUC:\", val_pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8894489c-30fa-4cd5-8438-c67c6f30f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 00:50:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:51:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:51:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.20  precision=0.248  recall=0.982  f1=0.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 00:51:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:51:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:51:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.30  precision=0.296  recall=0.965  f1=0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 00:51:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:51:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:51:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.40  precision=0.342  recall=0.945  f1=0.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 00:51:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:51:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:51:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "[Stage 2079:================================================>     (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.50  precision=0.387  recall=0.913  f1=0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "val_probs = (\n",
    "    val_preds_cls\n",
    "    .withColumn(\"prob_array\", vector_to_array(\"probability\"))\n",
    "    .select(\"is_surge\", \"prob_array\")\n",
    ")\n",
    "eps = 1e-9\n",
    "\n",
    "def metrics_for_threshold(th):\n",
    "    preds_th = val_probs.withColumn(\n",
    "        \"pred_is_surge\",\n",
    "        (F.col(\"prob_array\")[1] > F.lit(th)).cast(\"int\")   # prob of class 1\n",
    "    )\n",
    "    \n",
    "    tp = preds_th.filter(\"pred_is_surge = 1 AND is_surge = 1\").count()\n",
    "    fp = preds_th.filter(\"pred_is_surge = 1 AND is_surge = 0\").count()\n",
    "    fn = preds_th.filter(\"pred_is_surge = 0 AND is_surge = 1\").count()\n",
    "    \n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "for th in thresholds:\n",
    "    p, r, f1 = metrics_for_threshold(th)\n",
    "    print(f\"th={th:.2f}  precision={p:.3f}  recall={r:.3f}  f1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90db8bd2-173f-4089-91be-4ce668d68eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 00:53:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:53:22 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:53:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "[Stage 2106:=================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL (validation) – th = 0.5\n",
      "Precision: 0.3872531200775359\n",
      "Recall:    0.9125562138624518\n",
      "F1:        0.5437570443063383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "best_th = 0.5  # chosen based on F1\n",
    "\n",
    "val_with_probs = val_preds_cls.withColumn(\n",
    "    \"prob_array\", vector_to_array(\"probability\")\n",
    ")\n",
    "\n",
    "val_with_preds = val_with_probs.withColumn(\n",
    "    \"pred_is_surge_gbt\",\n",
    "    (F.col(\"prob_array\")[1] > F.lit(best_th)).cast(\"int\")\n",
    ")\n",
    "\n",
    "eps = 1e-9\n",
    "\n",
    "tp = val_with_preds.filter(\"pred_is_surge_gbt = 1 AND is_surge = 1\").count()\n",
    "fp = val_with_preds.filter(\"pred_is_surge_gbt = 1 AND is_surge = 0\").count()\n",
    "fn = val_with_preds.filter(\"pred_is_surge_gbt = 0 AND is_surge = 1\").count()\n",
    "\n",
    "precision = tp / (tp + fp + eps)\n",
    "recall    = tp / (tp + fn + eps)\n",
    "f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "\n",
    "print(\"FINAL (validation) – th =\", best_th)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:   \", recall)\n",
    "print(\"F1:       \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb93c4aa-c935-4ade-978c-9817ec1aa3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 00:53:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:54:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/12/01 00:54:23 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "[Stage 2133:===================================================>  (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST – th = 0.5\n",
      "Precision: 0.4530585524779987\n",
      "Recall:    0.9102003364428719\n",
      "F1:        0.6049827365038947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get test predictions from your saved/loaded model\n",
    "test_preds_cls = gbt_cls_model.transform(test_df)\n",
    "\n",
    "test_with_probs = test_preds_cls.withColumn(\n",
    "    \"prob_array\", vector_to_array(\"probability\")\n",
    ")\n",
    "\n",
    "test_with_preds = test_with_probs.withColumn(\n",
    "    \"pred_is_surge_gbt\",\n",
    "    (F.col(\"prob_array\")[1] > F.lit(best_th)).cast(\"int\")\n",
    ")\n",
    "\n",
    "tp_t = test_with_preds.filter(\"pred_is_surge_gbt = 1 AND is_surge = 1\").count()\n",
    "fp_t = test_with_preds.filter(\"pred_is_surge_gbt = 1 AND is_surge = 0\").count()\n",
    "fn_t = test_with_preds.filter(\"pred_is_surge_gbt = 0 AND is_surge = 1\").count()\n",
    "\n",
    "precision_t = tp_t / (tp_t + fp_t + eps)\n",
    "recall_t    = tp_t / (tp_t + fn_t + eps)\n",
    "f1_t        = 2 * precision_t * recall_t / (precision_t + recall_t + eps)\n",
    "\n",
    "print(\"TEST – th =\", best_th)\n",
    "print(\"Precision:\", precision_t)\n",
    "print(\"Recall:   \", recall_t)\n",
    "print(\"F1:       \", f1_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fabbf0e-6c8d-4652-9629-1084b9333f96",
   "metadata": {},
   "source": [
    "## Model Insights from Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102f6b1-acba-4f8e-9381-50f065a8d99a",
   "metadata": {},
   "source": [
    "**Recall is extremely high (91%)**\n",
    "\n",
    "The model correctly identifies 9 out of 10 actual surge periods, which is exactly what we’d want in a real ride-hailing system:\n",
    "- Missing true surges leads to underpricing + lost revenue + unhappy drivers\n",
    "- Flagging too many false surges leadss to light inefficiencies, but tolerable\n",
    "\n",
    "So high recall is desirable.\n",
    "\n",
    "**Precision is very reasonable (45%) in an imbalanced setting**\n",
    "\n",
    "Given surge is only ~8–10% of the data, a naïve model might have precision below 10%.\n",
    "\n",
    "45% precision is actually very good:\n",
    "- When the model predicts “surge”, 45% of the time it’s correct.\n",
    "- That’s more than 4× better than random guessing at the base rate.\n",
    "\n",
    "**F1 score (0.605) is strong**\n",
    "\n",
    "An F1 ≈ 0.60 is quite solid for:\n",
    "- imbalanced data\n",
    "- time-series structured\n",
    "- noisy weather + event + taxi interactions\n",
    "- only half-hour granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f27f8-2d3a-4912-8721-2683417e2368",
   "metadata": {},
   "source": [
    "### Surge Prediction Model Results\n",
    "\n",
    "We evaluated the final surge classifier on the held-out test set using the tuned threshold of 0.50, selected during validation based on the F1 score. The model achieved:\n",
    "- Precision: 0.453\n",
    "- Recall: 0.910\n",
    "- F1 Score: 0.605\n",
    "\n",
    "Despite the underlying class imbalance (surge represents only ~9% of all half-hour × zone observations), the model demonstrates strong performance. The high recall indicates that the classifier successfully identifies the vast majority of actual surge periods (≈91%), which aligns with the operational objective of minimizing missed surges. While precision is lower (≈45%), this is substantially above the surge base rate and reflects acceptable trade-offs: falsely predicting a surge primarily leads to conservative staffing or pricing adjustments, whereas failing to detect a true surge can cause supply shortages and revenue loss.\n",
    "\n",
    "The F1 score of 0.605 represents a solid balance between precision and recall and shows no evidence of overfitting, improving from validation (0.544) to test (0.605). This suggests that the model generalizes well and captures meaningful signals from weather, events, and lagged taxi activity patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12c0ba-1ada-421a-b110-c816003b6405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
